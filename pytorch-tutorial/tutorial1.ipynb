{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # 파이썬의 기본 내장함수로 시간관련 라이브러리\n",
    "import matplotlib.pyplot as plt # 시각화를 위환 라이브러리 \n",
    "import numpy as np # 다양한 계산 ex)행렬과 같은 계산을 할 수 있게 해주는 라이브러리\n",
    "import torch #pytorch 인공지능 및 딥러닝 모델을 구축하고 학습하기 위한 오픈 소스 딥러닝 라이브러리\n",
    "import torch.nn as nn # neural network의 약자로, 신경망 모델을 구성하는 데 사용되는 다양한 레이어, 손실 함수, 초기화 함수등을 포함하는 라이브러리\n",
    "import torch.utils.data as data # pytorch에서 데이터셋을 로드하고 전처리하는데 사용되는 유틸리티 함수와 클래스를 제공하는 함수\n",
    "from matplotlib.colors import to_rgba # 그래프에 색깔 넣기 위한 도구\n",
    "from torch import Tensor # 텐서를 많이 쓸것 같으니 그냥 클래스 하나를 따로 import함\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(\"Using torch\", torch.__version__) # torch버전 확인 \n",
    "#+cu117 ==cuda 11.7 과 호환되도록 빌드되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f041810a4b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) \n",
    "# 시드를 설정하여 난수를 생성할 때 동일한 난수가 나오게 만듬\n",
    "# 동일한 난수를 생성? -> 난수를 생성한다는게 무작위 수를 만든다는게 아니라 특정 알고리즘에 의해 어떠한 수를 생성하는거임 근데 시드42를 기반으로 알고리즘을 돌려준다고 하면 게속 같은 난수가 나오는거\n",
    "# 시드? -> 난수 생성 과정에서 시작점을 지정하는 역할, 그냥 초기값이라 생각하면 됨\n",
    "# 왜 이런짓을? -> 재현가능하게 만들어야 하는데 돌릴때마다 결과값이 달라지면 제대로 하고 있는지 헷갈려서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 3.8990e-02,  4.5566e-41, -5.1625e-12,  6.0646e+13],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.7060e-11,  4.5565e-41,  9.1084e-44,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "x =Tensor(2,3,4) # 3*4 행렬 2개 \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor는 pytorch에서 다차원 배열을 나타내는 클래스임\n",
    "이 클래스를 이용하면 새로운 텐서를 생성할 때, 이미 메모리에 할당되어 있는 값을 재사용할 수 있음.\n",
    "ex) a = 10\n",
    "    b = a (a가 참조하는 값 10을 재사용)\n",
    "이렇게 하면 메모리 절약 이득 && 귀찮게 다시 할당할 필요도 없음 :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "x2\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "x3\n",
      "tensor([[[0.8823, 0.9150, 0.3829],\n",
      "         [0.9593, 0.3904, 0.6009]],\n",
      "\n",
      "        [[0.2566, 0.7936, 0.9408],\n",
      "         [0.1332, 0.9346, 0.5936]],\n",
      "\n",
      "        [[0.8694, 0.5677, 0.7411],\n",
      "         [0.4294, 0.8854, 0.5739]]])\n",
      "x4\n",
      "tensor([[[-0.4974,  0.4396, -0.7581],\n",
      "         [ 1.0783,  0.8008,  1.6806]],\n",
      "\n",
      "        [[ 0.3559, -0.6866,  1.5736],\n",
      "         [-0.8455,  1.3123,  0.6872]],\n",
      "\n",
      "        [[-1.0892, -0.3553, -0.9138],\n",
      "         [-0.6581,  0.0499,  2.2667]],\n",
      "\n",
      "        [[ 1.1790, -0.4345, -1.3864],\n",
      "         [-1.2862, -1.4032,  0.0360]]])\n",
      "x5\n",
      "tensor([3, 5])\n",
      "x6\n",
      "tensor([[[4.0436e-34, 0.0000e+00, 4.0555e-02],\n",
      "         [4.5566e-41, 4.0438e-34, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 7.0065e-45, 0.0000e+00],\n",
      "         [0.0000e+00, 1.6605e-42, 0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.zeros(1,2,3) # 0으로 가득찬 텐서\n",
    "x2 = torch.ones(2,2,3) # 1로 가득찬 텐서\n",
    "x3 = torch.rand(3,2,3) # 0과 1 사이에서 균일하게 샘플링된 임의의 값으로 채워진 텐서\n",
    "x4 = torch.randn(4,2,3) # 평균이 0 이고 분산이 1인 정규 분포에서 샘플링된 임의의 값으로 체워진 텐서\n",
    "x5 = torch.arange(3,7,2) # 정해진 규칙으로 텐서를 생성 (시작, 끝, 몇 칸씩 움직일 건지)\n",
    "x6 = torch.Tensor(2,2,3) # 자기 멋대로 값을 채운 텐서 생성\n",
    "# 샘플링? -> 통계학 또는 확률론에서 주어진 분포로부터 데이터 포인트를 추출하는 과정\n",
    "# 데이터 포인트? - > 데이터 집합에서 개별적으로 측정된 하나의 관측값\n",
    "for i in range(1,7):\n",
    "    xi = eval('x' + str(i)) #eval()문자열을 그냥 코드로 보고 실행시킴, 보안상 안 좋으니 외부 입력이 있을 시 사용 하지 말 것!\n",
    "    print('x{}\\n{}'.format(i,xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서란?\n",
    "다차원 배열을 표현하기 위한 데이터 구조\n",
    "벡터, 행렬, 같은 것들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([[1,2], [3,4]]) # 리스트 두개를 하나로 묶어서 텐서를 생성할 수 있다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 3, 4])\n",
      "Type: <class 'torch.Size'>\n",
      "Size: torch.Size([2, 3, 4])\n",
      "Type: <class 'torch.Size'>\n",
      "Size: 2 3 4\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "shape = x.shape \n",
    "print(\"Shape:\", x.shape)\n",
    "print(\"Type:\",type(shape))\n",
    "\n",
    "size = x.size()\n",
    "print(\"Size:\", size)\n",
    "print(\"Type:\",type(size))\n",
    "dim1, dim2, dim3 = x.size()\n",
    "\n",
    "print(\"Size:\", dim1, dim2, dim3)\n",
    "\n",
    "# shape와 size 모두 텐서의 모양을 알려준다\n",
    "# 옛날에는 몰라도 지금은 타입도 같고 출력 형태도 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array: \n",
      " [[1 2]\n",
      " [3 4]]\n",
      "PyTorch tensor: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "cpu\n",
      "convert tnesor to numpy: \n",
      " [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array([[1,2], [3,4]]) \n",
    "tensor = torch.from_numpy(np_arr) # numpy를 tensor로 전환\n",
    "\n",
    "print(\"Numpy array:\", '\\n', np_arr) \n",
    "print(\"PyTorch tensor:\",'\\n', tensor)\n",
    "print(tensor.device) # 텐서가 gpu에 있는지 cpu에 있는지 출력\n",
    "\n",
    "tensor = tensor.numpy() # 텐서를 numpy로 변환하기 위해서는 텐서가 cpu에 있어야 한다, ex) np_arr = tensor.cpu().numpy()\n",
    "print(\"convert tnesor to numpy:\", '\\n', tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "\n",
      "x2 tensor([[0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "y tensor([[1.1388, 1.7086, 1.3236],\n",
      "        [1.0925, 1.3250, 1.1945]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # x1,x2 각 텐서들의 원소들이 항상 같은 수들로 채워짐\n",
    "x1 = torch.rand(2,3)\n",
    "x2 = torch.rand(2,3)\n",
    "y = x1 + x2 # 같은 위치에 있는 원소끼리 더함\n",
    "\n",
    "print(\"x1\", x1)\n",
    "print()\n",
    "print(\"x2\", x2)\n",
    "print()\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([[0.8694, 0.5677, 0.7411],\n",
      "        [0.4294, 0.8854, 0.5739]])\n",
      "x2: tensor([[0.2666, 0.6274, 0.2696],\n",
      "        [0.4414, 0.2969, 0.8317]])\n",
      "\n",
      "x2 add to x1: \n",
      " tensor([[1.1360, 1.1952, 1.0107],\n",
      "        [0.8708, 1.1824, 1.4056]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2,3)\n",
    "x2 = torch.rand(2,3)\n",
    "\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print()\n",
    "x2.add_(x1) # 텐서의 메모리에 직접 적용되는 내부연산, 내부연산은 일반적으로 함수명에 _을 붙여준다\n",
    "print(\"x2 add to x1:\",'\\n', x2) # x2를 호출해도 x1과 합쳐지기 전이 아닌 합쳐진 x2가 출력된다는걸 말하고 싶었나 봄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0, 1, 2, 3, 4, 5])\n",
      "x: tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "x tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "print(\"x:\", x)\n",
    "\n",
    "x = x.view(2,3) # 2행 3열로\n",
    "print(\"x:\", x)\n",
    "\n",
    "x = x.permute(1,0) # transpose\n",
    "print(\"x\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "W tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "h tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "x = x.view(2,3)\n",
    "print(\"X\", x)\n",
    "\n",
    "W = torch.arange(9).view(3, 3)\n",
    "print(\"W\", W)\n",
    "\n",
    "h = torch.matmul(x,W) # 행렬 곱\n",
    "print(\"h\", h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([1, 5, 9])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([3, 7])\n",
      "tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view(3,4)\n",
    "print(\"X\", x)\n",
    "print(x[:, 1]) # 각 행의 1번째 요소\n",
    "print(x[0]) # 0번째 행\n",
    "print(x[:2, -1]) # 0~1까지 행에서 마지막 요소\n",
    "print(x[1:3, :]) # 1~2까지 행에서 처음부터 끝까지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch는 우리가 정의한 함수의 기울기/도함수를 자동으로 계산해 줄 수 있다\n",
    "-> 왜 이게 필요한가? -> 우리가 어떤 것을 예측할 때 딥러닝을 사용 -> 어? 그런데 예측값이 실제랑 너무 차이나네...-> 문제는 가중치가 잘못 설정 된거 -> 그러면 가중치를 어떻게 잘 맞출 수 있을까? ->바로 그래디언트(다변수 함수의 편미분 벡터, 즉 함수의 값이 가장 빠르게 증가하는 방향!) (그냥 다변수 함수에서의 기울기라 생각) -> 기울기는 왜? -> 실제 값과 예측값의 차를 그래프로 그린 후 그 최소점을 가중치로 설정할건데 그 때 기울기가 0이 되는 부분이 최소점이 되는거임-> 근데 극소값이 없는 그래프는? -> 그러면 기울기가 가장크게 증가 또는 감소 하는 방향을 찾아서 그나마 제일 작은 값을 가중치로 쓰는거지 뭐 어쩌겠으"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,))\n",
    "print(x.requires_grad) # gradient 필요한지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True) # 그래디언트 필요하다고 \n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # 0~3까지 ,실수형으로 그래디언트 사용할 수 있게 만들어\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "b tensor([ 4.,  9., 16.], grad_fn=<PowBackward0>)\n",
      "c tensor([ 7., 12., 19.], grad_fn=<AddBackward0>)\n",
      "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n",
      "tensor([4., 6., 8.])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2393/1544413661.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  print(a.grad)\n"
     ]
    }
   ],
   "source": [
    "a = x + 2\n",
    "print(\"a\",a)\n",
    "b = a**2\n",
    "print(\"b\",b)\n",
    "c = b + 3\n",
    "print(\"c\",c)\n",
    "y = c.mean() # 각 요소들을 더하고 요소의 수만큼 나눔 \n",
    "print(\"Y\", y)\n",
    "y.backward()  \n",
    "# y에 대한 역전파(backpropagation)를 수행하며 y에 대한 gredient를 계산\n",
    "# 이 값에 따라 .requires_grad(True)를 호출한 텐서 x의 그래디언트도 계산되어 x.grad에 저장\n",
    "# .backward() 함수는 스칼라 출력에 대해서만 자동으로 그래디언트를 계산할 수 있다.\n",
    "print(x.grad)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
