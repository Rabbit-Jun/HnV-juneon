{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # 파이썬의 기본 내장함수로 시간관련 라이브러리\n",
    "import matplotlib.pyplot as plt # 시각화를 위환 라이브러리 \n",
    "import numpy as np # 다양한 계산 ex)행렬과 같은 계산을 할 수 있게 해주는 라이브러리\n",
    "import torch #pytorch 인공지능 및 딥러닝 모델을 구축하고 학습하기 위한 오픈 소스 딥러닝 라이브러리\n",
    "import torch.nn as nn # neural network의 약자로, 신경망 모델을 구성하는 데 사용되는 다양한 레이어, 손실 함수, 초기화 함수등을 포함하는 라이브러리\n",
    "import torch.utils.data as data # pytorch에서 데이터셋을 로드하고 전처리하는데 사용되는 유틸리티 함수와 클래스를 제공하는 함수\n",
    "from matplotlib.colors import to_rgba # 그래프에 색깔 넣기 위한 도구\n",
    "from torch import Tensor # 텐서를 많이 쓸것 같으니 그냥 클래스 하나를 따로 import함\n",
    "import torch.nn.functional as F #신경망 구성에 필요한 다양한 함수를 제공하는 모듈로, 활성화 함수 (ReLU, sigmoid,hanh 등), 컨볼루션 연산, 풀링 연산 등 다양한 연산을 포함한 라이브러리 내의 함수형 API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(\"Using torch\", torch.__version__) # torch버전 확인 \n",
    "#+cu117 ==cuda 11.7 과 호환되도록 빌드되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0399ee34b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) \n",
    "# 시드를 설정하여 난수를 생성할 때 동일한 난수가 나오게 만듬\n",
    "# 동일한 난수를 생성? -> 난수를 생성한다는게 무작위 수를 만든다는게 아니라 특정 알고리즘에 의해 어떠한 수를 생성하는거임 근데 시드42를 기반으로 알고리즘을 돌려준다고 하면 게속 같은 난수가 나오는거\n",
    "# 시드? -> 난수 생성 과정에서 시작점을 지정하는 역할, 그냥 초기값이라 생각하면 됨\n",
    "# 왜 이런짓을? -> 재현가능하게 만들어야 하는데 돌릴때마다 결과값이 달라지면 제대로 하고 있는지 헷갈려서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.0371e-01,  4.5565e-41,  3.2189e-34,  0.0000e+00],\n",
      "         [-3.3976e-01,  4.5565e-41,  8.8701e-10,  4.5565e-41],\n",
      "         [-3.8518e-01,  4.5565e-41,  8.8692e-10,  4.5565e-41]],\n",
      "\n",
      "        [[-3.8525e-01,  4.5565e-41, -3.6983e-01,  4.5565e-41],\n",
      "         [ 8.8702e-10,  4.5565e-41,  8.8560e-10,  4.5565e-41],\n",
      "         [-3.8519e-01,  4.5565e-41, -3.8893e-01,  4.5565e-41]]])\n"
     ]
    }
   ],
   "source": [
    "x =Tensor(2,3,4) # 3*4 행렬 2개 \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor는 pytorch에서 다차원 배열을 나타내는 클래스임\n",
    "이 클래스를 이용하면 새로운 텐서를 생성할 때, 이미 메모리에 할당되어 있는 값을 재사용할 수 있음.\n",
    "ex) a = 10\n",
    "    b = a (a가 참조하는 값 10을 재사용)\n",
    "이렇게 하면 메모리 절약 이득 && 귀찮게 다시 할당할 필요도 없음 :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "x2\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "x3\n",
      "tensor([[[0.8823, 0.9150, 0.3829],\n",
      "         [0.9593, 0.3904, 0.6009]],\n",
      "\n",
      "        [[0.2566, 0.7936, 0.9408],\n",
      "         [0.1332, 0.9346, 0.5936]],\n",
      "\n",
      "        [[0.8694, 0.5677, 0.7411],\n",
      "         [0.4294, 0.8854, 0.5739]]])\n",
      "x4\n",
      "tensor([[[-0.4974,  0.4396, -0.7581],\n",
      "         [ 1.0783,  0.8008,  1.6806]],\n",
      "\n",
      "        [[ 0.3559, -0.6866,  1.5736],\n",
      "         [-0.8455,  1.3123,  0.6872]],\n",
      "\n",
      "        [[-1.0892, -0.3553, -0.9138],\n",
      "         [-0.6581,  0.0499,  2.2667]],\n",
      "\n",
      "        [[ 1.1790, -0.4345, -1.3864],\n",
      "         [-1.2862, -1.4032,  0.0360]]])\n",
      "x5\n",
      "tensor([3, 5])\n",
      "x6\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  3.2395e-34],\n",
      "         [ 0.0000e+00, -3.9378e-01,  4.5565e-41]]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.zeros(1,2,3) # 0으로 가득찬 텐서\n",
    "x2 = torch.ones(2,2,3) # 1로 가득찬 텐서\n",
    "x3 = torch.rand(3,2,3) # 0과 1 사이에서 균일하게 샘플링된 임의의 값으로 채워진 텐서\n",
    "x4 = torch.randn(4,2,3) # 평균이 0 이고 분산이 1인 정규 분포에서 샘플링된 임의의 값으로 체워진 텐서\n",
    "x5 = torch.arange(3,7,2) # 정해진 규칙으로 텐서를 생성 (시작, 끝, 몇 칸씩 움직일 건지)\n",
    "x6 = torch.Tensor(2,2,3) # 자기 멋대로 값을 채운 텐서 생성\n",
    "# 샘플링? -> 통계학 또는 확률론에서 주어진 분포로부터 데이터 포인트를 추출하는 과정\n",
    "# 데이터 포인트? - > 데이터 집합에서 개별적으로 측정된 하나의 관측값\n",
    "for i in range(1,7):\n",
    "    xi = eval('x' + str(i)) #eval()문자열을 그냥 코드로 보고 실행시킴, 보안상 안 좋으니 외부 입력이 있을 시 사용 하지 말 것!\n",
    "    print('x{}\\n{}'.format(i,xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서란?\n",
    "다차원 배열을 표현하기 위한 데이터 구조\n",
    "벡터, 행렬, 같은 것들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([[1,2], [3,4]]) # 리스트 두개를 하나로 묶어서 텐서를 생성할 수 있다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 3, 4])\n",
      "Type: <class 'torch.Size'>\n",
      "Size: torch.Size([2, 3, 4])\n",
      "Type: <class 'torch.Size'>\n",
      "Size: 2 3 4\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "shape = x.shape \n",
    "print(\"Shape:\", x.shape)\n",
    "print(\"Type:\",type(shape))\n",
    "\n",
    "size = x.size()\n",
    "print(\"Size:\", size)\n",
    "print(\"Type:\",type(size))\n",
    "dim1, dim2, dim3 = x.size()\n",
    "\n",
    "print(\"Size:\", dim1, dim2, dim3)\n",
    "\n",
    "# shape와 size 모두 텐서의 모양을 알려준다\n",
    "# 옛날에는 몰라도 지금은 타입도 같고 출력 형태도 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array: \n",
      " [[1 2]\n",
      " [3 4]]\n",
      "PyTorch tensor: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "cpu\n",
      "convert tnesor to numpy: \n",
      " [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array([[1,2], [3,4]]) \n",
    "tensor = torch.from_numpy(np_arr) # numpy를 tensor로 전환\n",
    "\n",
    "print(\"Numpy array:\", '\\n', np_arr) \n",
    "print(\"PyTorch tensor:\",'\\n', tensor)\n",
    "print(tensor.device) # 텐서가 gpu에 있는지 cpu에 있는지 출력\n",
    "\n",
    "tensor = tensor.numpy() # 텐서를 numpy로 변환하기 위해서는 텐서가 cpu에 있어야 한다, ex) np_arr = tensor.cpu().numpy()\n",
    "print(\"convert tnesor to numpy:\", '\\n', tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "\n",
      "x2 tensor([[0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "y tensor([[1.1388, 1.7086, 1.3236],\n",
      "        [1.0925, 1.3250, 1.1945]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # x1,x2 각 텐서들의 원소들이 항상 같은 수들로 채워짐\n",
    "x1 = torch.rand(2,3)\n",
    "x2 = torch.rand(2,3)\n",
    "y = x1 + x2 # 같은 위치에 있는 원소끼리 더함\n",
    "\n",
    "print(\"x1\", x1)\n",
    "print()\n",
    "print(\"x2\", x2)\n",
    "print()\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([[0.8694, 0.5677, 0.7411],\n",
      "        [0.4294, 0.8854, 0.5739]])\n",
      "x2: tensor([[0.2666, 0.6274, 0.2696],\n",
      "        [0.4414, 0.2969, 0.8317]])\n",
      "\n",
      "x2 add to x1: \n",
      " tensor([[1.1360, 1.1952, 1.0107],\n",
      "        [0.8708, 1.1824, 1.4056]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2,3)\n",
    "x2 = torch.rand(2,3)\n",
    "\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print()\n",
    "x2.add_(x1) # 텐서의 메모리에 직접 적용되는 내부연산, 내부연산은 일반적으로 함수명에 _을 붙여준다\n",
    "print(\"x2 add to x1:\",'\\n', x2) # x2를 호출해도 x1과 합쳐지기 전이 아닌 합쳐진 x2가 출력된다는걸 말하고 싶었나 봄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0, 1, 2, 3, 4, 5])\n",
      "x: tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "x tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "print(\"x:\", x)\n",
    "\n",
    "x = x.view(2,3) # 2행 3열로\n",
    "print(\"x:\", x)\n",
    "\n",
    "x = x.permute(1,0) # transpose\n",
    "print(\"x\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "W tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "h tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "x = x.view(2,3)\n",
    "print(\"X\", x)\n",
    "\n",
    "W = torch.arange(9).view(3, 3)\n",
    "print(\"W\", W)\n",
    "\n",
    "h = torch.matmul(x,W) # 행렬 곱\n",
    "print(\"h\", h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([1, 5, 9])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([3, 7])\n",
      "tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view(3,4)\n",
    "print(\"X\", x)\n",
    "print(x[:, 1]) # 각 행의 1번째 요소\n",
    "print(x[0]) # 0번째 행\n",
    "print(x[:2, -1]) # 0~1까지 행에서 마지막 요소\n",
    "print(x[1:3, :]) # 1~2까지 행에서 처음부터 끝까지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch는 우리가 정의한 함수의 기울기/도함수를 자동으로 계산해 줄 수 있다\n",
    "-> 왜 이게 필요한가? -> 우리가 어떤 것을 예측할 때 딥러닝을 사용 -> 어? 그런데 예측값이 실제랑 너무 차이나네...-> 문제는 가중치가 잘못 설정 된거 -> 그러면 가중치를 어떻게 잘 맞출 수 있을까? ->바로 그래디언트(다변수 함수의 편미분 벡터, 즉 함수의 값이 가장 빠르게 증가하는 방향!) (그냥 다변수 함수에서의 기울기라 생각) -> 기울기는 왜? -> 실제 값과 예측값의 차를 그래프로 그린 후 그 최소점을 가중치로 설정할건데 그 때 기울기가 0이 되는 부분이 최소점이 되는거임-> 근데 극소값이 없는 그래프는? -> 그러면 기울기가 가장크게 증가 또는 감소 하는 방향을 찾아서 그나마 제일 작은 값을 가중치로 쓰는거지 뭐 어쩌겠으"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,))\n",
    "print(x.requires_grad) # gradient 필요한지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True) # 그래디언트 필요하다고 \n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # 0~3까지 ,실수형으로 그래디언트 사용할 수 있게 만들어\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "b tensor([ 4.,  9., 16.], grad_fn=<PowBackward0>)\n",
      "c tensor([ 7., 12., 19.], grad_fn=<AddBackward0>)\n",
      "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 2.0000, 2.6667])\n"
     ]
    }
   ],
   "source": [
    "a = x + 2\n",
    "print(\"a\",a)\n",
    "b = a**2\n",
    "print(\"b\",b)\n",
    "c = b + 3\n",
    "print(\"c\",c)\n",
    "y = c.mean() # 각 요소들을 더하고 요소의 수만큼 나눔 \n",
    "print(\"Y\", y)\n",
    "y.backward()  \n",
    "# y에 대한 역전파(backpropagation)를 수행하며 y에 대한 gredient를 계산\n",
    "# 이 값에 따라 .requires_grad(True)를 호출한 텐서 x의 그래디언트도 계산되어 x.grad에 저장\n",
    "# .backward() 함수는 스칼라 출력에 대해서만 자동으로 그래디언트를 계산할 수 있다.\n",
    "print(x.grad) # x의 미분값... 어케 구했는데...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y를 x에 대한 함수로 보고 미분 값을 구한다.\n",
    "1. y = 1/3*c -> (y)'= 1/3\n",
    "2. c = b + 3 -> (c)'= 1\n",
    "3. b = a **2 -> (b)' = 2a\n",
    "4. a = x + 2 -> (a)' = 1\n",
    "즉, y =1/3[{(x+2)**2}+3]= 1/3(x^2+4x+7) -> (y)' 2/3x+4/3  (전개해서 미분) == 1/3*1*2a*1 =2a/3 =2x/3+4/3 (연쇄법칙으로 미분)\n",
    "그래디언트는 2a/3 = 2/3*[2.,3.,4.] ==[1.3333, 2., 2.6667]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS the GPU availabe? True\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available() # gpu사용 가능한지\n",
    "print(f\"IS the GPU availabe? {gpu_avail}\")\n",
    "#컴퓨터에 gpu가 있는데 안된다고 한다면 CUDA버전이 올바르게 설치 되어있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# 만약 쿠다 사용가능하면 쿠다를 device에 아니면 cpu를 device에 저장해라\n",
    "# 왜 이런짓을 할까? -> 그것은 텐서는 기본적으로 cpu에 저장되는데 간단한 연산처리를 많이 할 때는 gpu가 더 빠르단 말이징\n",
    "# 그러니 gpu를 쓸 수 있을 땐 gpu를 쓸려고 미리 초기화 시켜두는거임, 나중에 if device == cuda 어쩌고 저쩌고 할려고, \n",
    "# 그냥 할때 device = torch.device(\"cuda\")를 쓰면 되잖아 -> 내가 할 때마다 gpu있는지 없는지 확인하고 또 코드짜기 귀찬잖아\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2,3)\n",
    "x = x.to(device) # device에 푸시 (device에 대해서는 윗 셀에 명시함)\n",
    "print(\"x:\",'\\n', x)\n",
    "# cuda:0 은 컴퓨터의 0번째 gpu장치라는 뜻\n",
    "# 그럼 2번째 3번째도 있음? -> 있음, gpu살 돈만 있다면 여러개 넣어서 병렬 계산으로 시간 단축 크게 가능 \\(>.<)/, 그리고 pytorch는 다중 gpu지원해줌 근데 매우매우 큰 학습이 필요한게 아니면 필요없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그러면 gpu랑 cpu랑 계산속도가 얼마나 차이가 날까?\n",
    "시간은 없을 때 얼마나 걸릴지 예측하며 희망을 갖는 습관은 매우 중요하므로 gpu와 cpu의 계산속도를 한번 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.21896s\n",
      "GPU time: 0.14421s\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5000, 5000) #정규분포에서 샘플링한 값으로 채워진 5000행 5000열의 행렬\n",
    "start_time = time.time() # 현재시간을 초 단위로 반환\n",
    "_ = torch.matmul(x, x) #텐서끼리 곱해라\n",
    "end_time = time.time() #현재시간을 초 단위로 반환\n",
    "print(f\"CPU time: {(end_time - start_time):6.5f}s\") # 끝 시간 - 시작시간을 전체길이 6 소수점 이하 5자리까지 표현해라\n",
    "\n",
    "if torch.cuda.is_available(): # 쿠다 사용 가능하면\n",
    "    x = x.to(device) # 쿠다에 푸쉬\n",
    "    start = torch.cuda.Event(enable_timing=True) # cuda 이벤트를 생성, 옵션으로 생성된 이벤트를 시간측정에 사용할 수 있게함\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record() # 연산이 시작되는 시점 표시\n",
    "    _ = torch.matmul(x, x) # 행렬 곱\n",
    "    end.record() # 연산이 끝나는 시점 표시\n",
    "    torch.cuda.synchronize() # gpu연산이 완료될 때가지 cpu가 기다리게 해라\n",
    "    # gpu는 비동기적 cpu는 동기적 -> gpu 연산 다 안끝났는데 cpu가 지 할거 다하고 나중에 gpu연산 끝난거 기록하거나 다 안 끝나는데 기록 할 수도 있음\n",
    "    print(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42) # cuda에 시드 설정 (cpu와 gpu간의 시드가 동기화 되지 않아요)\n",
    "    torch.cuda.manual_seed_all(42) # 모든 cuda 장치에 시드 설정\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# CuDNN(테슬라GPU를 위한 NVIDA의 딥 네트워크 라이브러리)의 동작을 결정록적으로 만들어 실험결과의 재현성 보장\n",
    "# 비결정적 알고리즘이 종종 결정론적 알고리즘보다 더 빠르게 작동하므로 이는 선택\n",
    "# NVIDA에서 만든 GPU라면 다 사용 가능\n",
    "# 비결정적 알고리즘이 뭔데? -> 주어진 입력에 대해 항상 동일한 결과를 생성하지 않는 알고리즘\n",
    "torch.backends.cudnn.venchmark = False\n",
    "# 각 레이어에 대해 여러가지 컨볼루션 알고리즘(수학적 연산 알고리즘)을 테스트하여 가장 빠른 알고리즘을 선택하는 기능을 끈다\n",
    "# 왜? -> 입력 데이터의 크기가 변하면 변경될 때마다 다시 테스트를 수행하는게 싫어서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module): # 파이토치에서 신경망을 구성하는 기본 단위 '모듈', nn.Module 클래스를 상속받은 사용자 정의 클래스를 의미한다\n",
    "    def __init__(self):\n",
    "        super().__init__ # 상위 클래스의 __init__메소드를 호출하기 위해 super() 사용\n",
    "\n",
    "    def forward(self, x): # 모듈의 계산이 이루어지는 곳\n",
    "        pass\n",
    "# 이 형태가 모듈의 기본 틀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(X)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_inputs: 입력층의 노드 수, 입력 데이터의 특성 수와 일치해야 한다\n",
    "num_hidden: 은닉층의 노드 수를 지정합니다.이 값은 사용자가 임의로 설정 가능\n",
    "num_outputs: 출력층의 노드 수를 지정, 이 값은 예측하려는 클래스 수와 일치해야 한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
